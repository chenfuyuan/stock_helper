import pytest
import logging
from src.modules.llm_platform.application.services.llm_service import LLMService
from src.modules.llm_platform.infrastructure.registry import LLMRegistry
from src.modules.llm_platform.infrastructure.persistence.repositories.pg_config_repo import PgLLMConfigRepository

# è®¾ç½®æ—¥å¿—ï¼Œæ–¹ä¾¿æŸ¥çœ‹è°ƒç”¨è¿‡ç¨‹
logger = logging.getLogger(__name__)

@pytest.mark.asyncio
class TestLLMRealCall:
    """
    å¤§æ¨¡å‹çœŸå®è°ƒç”¨é›†æˆæµ‹è¯•ç±»ã€‚
    ä½¿ç”¨æ•°æ®åº“ä¸­å·²æœ‰çš„é…ç½®è¿›è¡Œå®é™…çš„ç½‘ç»œè¯·æ±‚æµ‹è¯•ã€‚
    """

    async def test_all_active_models_call(self, db_session):
        """
        æµ‹è¯•æ•°æ®åº“ä¸­æ‰€æœ‰æ¿€æ´»çš„æ¨¡å‹æ˜¯å¦éƒ½èƒ½æ­£å¸¸è°ƒç”¨ã€‚
        """
        # 1. åˆå§‹åŒ–åŸºç¡€è®¾æ–½
        repo = PgLLMConfigRepository(db_session)
        registry = LLMRegistry()
        registry.set_repository(repo)
        
        # 2. ä»æ•°æ®åº“åŠ è½½é…ç½®åˆ°æ³¨å†Œä¸­å¿ƒ
        await registry.refresh()
        
        configs = registry.get_all_configs()
        active_configs = [c for c in configs if c.is_active]
        
        if not active_configs:
            pytest.skip("æ•°æ®åº“ä¸­æ²¡æœ‰æ¿€æ´»çš„å¤§æ¨¡å‹é…ç½®ï¼Œè·³è¿‡æµ‹è¯•ã€‚è¯·å…ˆä½¿ç”¨ scripts/add_llm_config_template.py æ·»åŠ é…ç½®ã€‚")

        # 3. åˆå§‹åŒ–åº”ç”¨æœåŠ¡
        service = LLMService(registry=registry)
        
        print(f"\nå‘ç° {len(active_configs)} ä¸ªæ¿€æ´»çš„æ¨¡å‹é…ç½®ï¼Œå¼€å§‹é€ä¸€æµ‹è¯•...")

        # 4. éå†æµ‹è¯•æ¯ä¸ªæ¨¡å‹
        success_count = 0
        failure_details = []

        for config in active_configs:
            print(f"\n[æµ‹è¯•æ¨¡å‹] Alias: {config.alias} | Vendor: {config.vendor} | Model: {config.model_name}")
            
            try:
                # æ‰§è¡Œä¸€æ¬¡ç®€å•çš„å¯¹è¯ç”Ÿæˆ
                response = await service.generate(
                    prompt="ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚",
                    alias=config.alias,
                    temperature=0.7
                )
                
                print(f"  âœ… è°ƒç”¨æˆåŠŸï¼")
                print(f"  ğŸ’¬ æ¨¡å‹å›å¤: {response}")
                
                assert response is not None
                assert len(response.strip()) > 0
                success_count += 1
                
            except Exception as e:
                print(f"  âŒ è°ƒç”¨å¤±è´¥: {str(e)}")
                failure_details.append(f"Model {config.alias} failed: {str(e)}")

        # 5. æ€»ç»“æµ‹è¯•ç»“æœ
        print(f"\næµ‹è¯•ç»“æŸ: æˆåŠŸ {success_count}/{len(active_configs)}")
        
        if failure_details:
            error_msg = "\n".join(failure_details)
            pytest.fail(f"éƒ¨åˆ†æ¨¡å‹è°ƒç”¨å¤±è´¥:\n{error_msg}")

    async def test_routing_by_tags(self, db_session):
        """
        æµ‹è¯•é€šè¿‡æ ‡ç­¾(Tags)è¿›è¡Œè·¯ç”±è°ƒç”¨ã€‚
        """
        repo = PgLLMConfigRepository(db_session)
        registry = LLMRegistry()
        registry.set_repository(repo)
        await registry.refresh()
        
        configs = registry.get_all_configs()
        if not configs:
            pytest.skip("æ•°æ®åº“ä¸­æ— é…ç½®")

        # å¯»æ‰¾å¸¦æœ‰æ ‡ç­¾çš„æ¨¡å‹
        tagged_configs = [c for c in configs if c.tags and c.is_active]
        if not tagged_configs:
            pytest.skip("æ•°æ®åº“ä¸­æ²¡æœ‰å¸¦æ ‡ç­¾çš„æ¿€æ´»æ¨¡å‹ï¼Œè·³è¿‡æ ‡ç­¾è·¯ç”±æµ‹è¯•ã€‚")

        service = LLMService(registry=registry)
        
        # å°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„ç¬¬ä¸€ä¸ªæ ‡ç­¾è¿›è¡Œè°ƒç”¨
        target_tag = tagged_configs[0].tags[0]
        print(f"\n[æµ‹è¯•æ ‡ç­¾è·¯ç”±] ä½¿ç”¨æ ‡ç­¾: {target_tag}")
        
        try:
            response = await service.generate(
                prompt="Ping",
                tags=[target_tag]
            )
            print(f"  âœ… é€šè¿‡æ ‡ç­¾ [{target_tag}] è°ƒç”¨æˆåŠŸï¼")
            assert response is not None
        except Exception as e:
            pytest.fail(f"æ ‡ç­¾è·¯ç”±è°ƒç”¨å¤±è´¥: {str(e)}")
